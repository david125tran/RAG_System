# ğŸ“š Retrieval-Augmented Generation (RAG) Systems ğŸ§ª
## About
* This repo is a sandbox for different RAG System projects.  
* Each folder `01`, `02`, etc. is a different project.  

---
## ğŸ“‚ 01 - SEC 10â€‘K filings RAG System ğŸ›ï¸
- **Project Overview:** An end-to-end Retrieval-Augmented Generation (RAG) system for querying SEC 10â€‘K filings. 
- **Dataset:** ğŸ”— [S&P 500 EDGAR 10-K (Hugging Face)](https://huggingface.co/datasets/jlohding/sp500-edgar-10k)
    - Original filings from SEC EDGAR; mirrored on Hugging Face for research use.
- **Parts:** 
    - **`Create_VectorDB.ipynb`** - Google Colab: Vector Store Builder - chunk + embed 10-Ks and write a persistent **ChromaDB** collection (`edgar_10k`) using `BAAI/bge-small-en-v1.5`. Built on a GPU runtime (A100) for speed.
    - **`edgar_10k_chroma_*`** - Download Vector Store - copy the Chroma folder from Colab to your local machine.
    - **`RAG Pipeline.py`** - Local Machine: RAG Pipeline - retrieve top chunks and answer with a small local LLM (`Qwen/Qwen2.5-0.5B-Instruct`) on CPU (my local machine that doesn't have GPU runtime).
- **Repo Layout:**
```
01/
â”œâ”€â”€ Create_VectorDB.ipynb
â”œâ”€â”€ RAG Pipeline.py
â””â”€â”€ VectorDB/
   â””â”€â”€ edgar_10k_chroma_*/
      â””â”€â”€ edgar_10k_chroma_YYYYMMDD_HHMMSS/               
      â””â”€â”€ chroma.sqlite3.sqlite3
```

- **Limitations (by design):** This project has some limitations that I'm aware of.  However, I was using this to play around with RAG concepts and learn more about different frameworks.  
    - Company parsing is heuristic (regex); sensitive to spelling, punctuation, and renames.
    - Tiny LLM (Qwen 0.5B) â†’ basic reasoning only; answers strictly from context.
---

## ğŸ“‚ 02 - Biomedical PubMed RAG Assistant ğŸ§¬  
- **Project Overview:**  
  A Retrieval-Augmented Generation (RAG) system that automatically retrieves biomedical research from PubMed based on a topic (e.g., *plasma metanephrines*), downloads abstracts, chunks and embeds them, builds a **FAISS** vector store, and answers domain-specific biomedical queries grounded in real literature.

- **Data Source:** ğŸ”— [PubMed E-Utilities API ](https://www.ncbi.nlm.nih.gov/home/develop/api/)
  - Articles retrieved programmatically via `esearch` + `efetch`.  
  - Abstracts parsed from XML and stored locally with metadata (PMID, source URL).

- **Parts:**  
  - **`RAG_PubMed_Pipeline.py`** - End-to-end script that:
    - Queries PubMed API  
    - Downloads/Stores abstracts  
    - Splits content â†’ embeddings â†’ **FAISS VectorDB**  
    - Runs a RAG pipeline using `ChatOpenAI` + LangChain retrieval  
    - Optionally visualizes embeddings using **PCA** in 3D or 4D
  - **`pubmed_abstracts/`** - Folder containing normalized article text files organized by search topic.
  - **`vectorstore_db/`** - Local FAISS vector store persisted to disk.

- **Repo Layout:**
```
02/
â”œâ”€â”€ RAG_PubMed_Pipeline.py
â”œâ”€â”€ pubmed_abstracts/
â”‚ â””â”€â”€ plasma metanephrine/
â”‚ â””â”€â”€ pmid_*.txt
â””â”€â”€ vectorstore_db/
â””â”€â”€ index.faiss + metadata
```
- **Limitations (by design):** I could only get access to the abstracts of research papers in PubMed.  Full-text access to articles varies by journal licensing, so some responses may lack deeper context available only in complete articles.  I only asked it questions that I knew came from the abstracts.  

---
## ğŸ“‚ 03 â€“ Infectious Disease RAG Assistant ğŸ¦ 

- **Project Overview:**  
  A production-style Retrieval-Augmented Generation (RAG) chat assistant focused on **infectious diseases**, built with a full frontend + backend architecture.  This system combines domain-specific RAG with a **semantic LLM response cache**, **allowing first-turn user questions that are semantically similar to reuse previously generated LLM responses** across users to explore cost savings with messaging LLMs on a large scale.

  This project emphasizes **real-world engineering concerns**: prompt-injection defense, response caching, usage logging, persistence, and UI/backend coordination.
- **Parts:**  
    - Domain-specific RAG - Retrieves infectious-disease context from a FAISS vector store
    - Semantic LLM Response Cache (MySQL + FAISS)
        - Exact-hash cache for repeated first-turn queries  
        - Cache usage is **intentionally limited to first-turn queries** to **avoid applying cached responses in contexts where prior conversation state could change the correct answer**.
        - Semantic cache using cosine similarity over normalized embeddings  
        - Prevents redundant LLM calls for semantically identical questions  
    - Prompt-Injection & Input Hardening
    - Async FastAPI backend orchestrating RAG, caching, and LLM calls  
    - Integration with Amazon Bedrock via async `converse` calls 
    - React-based chat UI
- **Repo Layout:**
```
infectious-disease-chat/
â”‚
â”œâ”€â”€ start-frontend.bat                                   # Launch React UI (npm start / vite / etc.)
â”œâ”€â”€ start-backend.bat                                    # Launch FastAPI backend (uvicorn backend.main:app)
â”‚
â”œâ”€â”€ rag pipeline/
â”‚   â”‚
â”‚   â”œâ”€â”€ knowledge base/
â”‚   â”‚   â””â”€â”€ *.txt, *.md, *.pdf, *.docx, *.html, *.htm    # Raw infectious disease reference documents
â”‚   â”‚
â”‚   â”œâ”€â”€ vectorstore_db/
â”‚   â”‚   â”œâ”€â”€ index.faiss                                  # FAISS vector index
â”‚   â”‚   â””â”€â”€ index.pkl                                    # Serialized metadata
â”‚   â”‚
â”‚   â””â”€â”€ vectorstore_builder.py                           # Script to embed documents & build FAISS DB
â”‚
â”œâ”€â”€ backend/
â”‚   â”‚
â”‚   â”œâ”€â”€ main.py                                          # FastAPI entry point (POST /api/chat)
â”‚   â”œâ”€â”€ aws_bedrock_client.py                            # Async AWS Bedrock chat wrapper
â”‚   â”œâ”€â”€ validate_input.py                                # Prompt-injection & safety validation
â”‚   â”‚
â”‚   â”œâ”€â”€ .env                                             # Environment variables:
â”‚   â”‚                                                     #  - AWS_ACCESS_KEY_ID
â”‚   â”‚                                                     #  - AWS_SECRET_ACCESS_KEY
â”‚   â”‚                                                     #  - OPENAI_API_KEY
â”‚   â”‚                                                     #  - BASE_MODEL
â”‚   â”‚                                                     #  - DB_USER / DB_PW / DB_DATABASE_NAME
â”‚   â”‚
â”‚   â””â”€â”€ __pycache__/                                     # Python bytecode cache
â”‚
â”œâ”€â”€ frontend/
â”‚   â”‚
â”‚   â”œâ”€â”€ package.json                                     # Frontend dependencies & scripts
â”‚   â”œâ”€â”€ node_modules/
â”‚   â”‚
â”‚   â”œâ”€â”€ public/
â”‚   â”‚   â”œâ”€â”€ index.html                                   # HTML shell
â”‚   â”‚   â””â”€â”€ avatar.png                                   # Assistant avatar
â”‚   â”‚
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ App.js                                       # Main chat UI (state, API calls, history)
â”‚       â”œâ”€â”€ App.css                                      # Chat UI styling
â”‚       â”œâ”€â”€ index.js                                     # React entry point â†’ App
â”‚       â””â”€â”€ index.css                                    # Global styles (resets, fonts)
â”‚
â””â”€â”€ directory.md                                         # Project directory overview (this file)
```
